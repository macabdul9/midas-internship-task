{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import praw\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Properties of a submission(post is often reffered as submission), I am manually eliminating few of them, bacuase flair of the submission is least (or not) dependent on them ie: Flair is rarely correlated to author name, or subreddit etc\n",
    "\n",
    "#### Submission Properties: dict_keys(['all_awardings', 'allow_live_comments', 'author', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_text', 'author_flair_type', 'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders', 'can_mod_post', 'contest_mode', 'created_utc', 'domain', 'full_link', 'gildings', 'id', 'is_crosspostable', 'is_meta', 'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video', 'link_flair_background_color', 'link_flair_css_class', 'link_flair_richtext', 'link_flair_template_id', 'link_flair_text', 'link_flair_text_color', 'link_flair_type', 'locked', 'media_only', 'no_follow', 'num_comments', 'num_crossposts', 'over_18', 'parent_whitelist_status', 'permalink', 'pinned', 'pwls', 'removed_by_category', 'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler', 'steward_reports', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_subscribers', 'subreddit_type', 'thumbnail', 'title', 'total_awards_received', 'url', 'whitelist_status', 'wls'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference: \n",
    "- https://praw.readthedocs.io/en/latest/index.html\n",
    "- https://github.com/pushshift/apia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating a reddit app instance to collect post data from r/india submreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='pw8WCM92ySUjsQ', client_secret='y0MJFMWBtHXMLW2-3B2upsU2jYQ', user_agent='reddit-scrap', username='macabdul9', password='Sudo$0#1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can get the submission either by id or url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = reddit.submission(id=\"fxqifi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "126"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "## test submission\n",
    "subm.num_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn Around Time(TAT) for getting all comments of a post is way higher using praw with submission_id than pushshift.io api comment search below code snippets illustrate the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# comment_ids = []\n",
    "# count = 0\n",
    "# for comment in subm.comments.list():\n",
    "#     comment_ids.append(comment.id)\n",
    "#     print(comment.body)\n",
    "#     count+=1\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# comments = getPushshiftSubData(','.join(comment_ids))\n",
    "# count = 0\n",
    "# for comment in comments:\n",
    "#     print(comment['body'])\n",
    "#     count +=1\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit API does not allow to scarp more than 1000 submissions per request hence scrapping large data using reddit api is not possible. Pushshift api https://github.com/pushshift/api allows to scrap unlimited amount of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function will receive url of a submission, num_commnents as arg and it will return the top 10 comment and mean comment score(upvote - downvote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(url, num_comments):\n",
    "    # call api using api wrapper\n",
    "    subm = reddit.submission(url=url)\n",
    "    comments_body = []\n",
    "    sum_score = 0\n",
    "    count = 0\n",
    "    if num_comments > 0:\n",
    "        for i, comment in enumerate(subm.comments.list()):\n",
    "\n",
    "            # There may be some comments which has no body\n",
    "            try : \n",
    "                comments_body.append(comment.body)\n",
    "            except:\n",
    "                comments_body.append('')\n",
    "\n",
    "            # There exist some comments which has not given a score\n",
    "            try:\n",
    "                sum_score += comment.score\n",
    "            except:\n",
    "                sum_score += 0            \n",
    "            count += 1\n",
    "            # We only need 10 comments\n",
    "            if (i+1)%10==0:\n",
    "                break\n",
    "    try:\n",
    "        mean = sum_score/count\n",
    "    except:\n",
    "        mean = 0.0\n",
    "\n",
    "    return \" \".join(comments_body), mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to call pushshift api and retrive the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushshiftSubData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=2000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties to extract \n",
    "- A custom property is a property derived from properties or can't be extracted using pushshift submission search endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'allow_live_comments', 'author', 'author_premium', 'can_mod_post', 'contest_mode', 'created_utc',\n",
    "    'full_link', 'id', 'is_crosspostable', 'is_meta', 'is_original_content', 'is_self', 'is_video',\n",
    "    'link_flair_text', 'locked', 'media_only', 'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
    "    'parent_whitelist_status', 'permalink', 'pinned', 'score', 'selftext', 'send_replies', 'spoiler',\n",
    "    'stickied', 'title', 'total_awards_received', 'url'\n",
    "]\n",
    "custom_features = [\n",
    "    'comments', 'mean_comment_score'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to extract the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectSubData(subm):\n",
    "    # extracting all information about a submission\n",
    "    feature_values = [subm.get(key) for key in features]    \n",
    "    comments, mean_comment_score = get_comments(subm.get('full_link'), subm.get('num_comments')) \n",
    "    feature_values.append(comments)\n",
    "    feature_values.append(mean_comment_score)\n",
    "    subStats[subm.get('id')] = feature_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Parameters and Store Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subreddit to query\n",
    "sub='india'\n",
    "\n",
    "#before and after dates\n",
    "after = \"1559347200\"  # Sat, 01 Jun 2019 00:00:00 \n",
    "before = \"1561939199\" # Sun, 30 Jun 2019 23:59:59  \n",
    "query = \"\" # title should have either null string more \n",
    "subCount = 0\n",
    "subStats = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the data from as per timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getPushshiftSubData(query, after, before, sub)# Will run until all posts have been gathered \n",
    "# from the 'after' date up until before date\n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        collectSubData(submission)\n",
    "        subCount+=1\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    # print(len(data))\n",
    "    if subCount % 1000 == 0:\n",
    "        print(f'{subCount} submission collected')\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    after = data[-1]['created_utc']\n",
    "    data = getPushshiftSubData(query, after, before, sub)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the json object into a pickle and/or store all the data into csv a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Submissions-June-2019.pkl\", \"wb\") as f:\n",
    "    pickle.dump(subStats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=list(subStats.values()), columns = features + custom_features, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Submissions-June-2019.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- To extract the large (or sufficient) amount of data above code [2. Data Acquisation] was executed in multiple python environments to decrease the amount of time taken to gather the data ie: To collect the submission of a month 2. [2. Data Acquisation] code was executed in seperate environment.\n",
    "- On the other hand multiple reddit api instances (one api app instance for each running environment) also help to reduce the time to make API request faster(multiple running threads makes the request from same instance subtantially increases the waiting time)\n",
    "- All submissions fromm Jan 1, 2019 to April 10, 2019 were collected for this task and stored into the csv files(pkl as well), one file for each month's data (1 file for March 1, 2020 to April 10, 2020)\n",
    "- A reliable internet connection is required for uninterrupted exectution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bitvenvvirtualenv6d4449f86c2241cf8286aec1481d1ccd",
   "display_name": "Python 3.7.0 64-bit ('venv': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}