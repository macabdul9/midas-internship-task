{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import praw\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Acquisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Properties of a submission, I am manually eliminating few of them, bacuase flair of the submission is least (or not) dependent on them ie: Flair is rarely dependent on author name, or subreddit etc\n",
    "\n",
    "##### All Properties: dict_keys(['all_awardings', 'allow_live_comments', 'author', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_text', 'author_flair_type', 'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders', 'can_mod_post', 'contest_mode', 'created_utc', 'domain', 'full_link', 'gildings', 'id', 'is_crosspostable', 'is_meta', 'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video', 'link_flair_background_color', 'link_flair_css_class', 'link_flair_richtext', 'link_flair_template_id', 'link_flair_text', 'link_flair_text_color', 'link_flair_type', 'locked', 'media_only', 'no_follow', 'num_comments', 'num_crossposts', 'over_18', 'parent_whitelist_status', 'permalink', 'pinned', 'pwls', 'removed_by_category', 'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler', 'steward_reports', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_subscribers', 'subreddit_type', 'thumbnail', 'title', 'total_awards_received', 'url', 'whitelist_status', 'wls'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference: \n",
    "- https://praw.readthedocs.io/en/latest/index.html\n",
    "- https://github.com/pushshift/apia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a reddit app instance to collect post data from r/india submreddit\n",
    "reddit = praw.Reddit(client_id='xVeb5-ej49aBDg', client_secret='HLuuBB0e1upw1pbZ-oUFtrBplFY', user_agent='reddit-scrap', username='macabdul9', password='Sudo$0#1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the submission either by id or url\n",
    "subm = reddit.submission(id=\"fxqifi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Submission' object has no attribute 'full_link'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d22a9800d4d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msubm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_link\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\praw\\models\\reddit\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attribute)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         raise AttributeError(\n\u001b[0;32m     36\u001b[0m             \"{!r} object has no attribute {!r}\".format(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\praw\\models\\reddit\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attribute)\u001b[0m\n\u001b[0;32m     35\u001b[0m         raise AttributeError(\n\u001b[0;32m     36\u001b[0m             \"{!r} object has no attribute {!r}\".format(\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             )\n\u001b[0;32m     39\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Submission' object has no attribute 'full_link'"
     ]
    }
   ],
   "source": [
    "subm.full_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turn Around Time(TAT) for getting all comments of a post is slightly higher than using praw with submission_id/url than pushshift.io api comment search below code snippets illustrate the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# comment_ids = []\n",
    "# count = 0\n",
    "# for comment in subm.comments.list():\n",
    "#     comment_ids.append(comment.id)\n",
    "#     print(comment.body)\n",
    "#     count+=1\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# comments = getPushshiftSubData(','.join(comment_ids))\n",
    "# count = 0\n",
    "# for comment in comments:\n",
    "#     print(comment['body'])\n",
    "#     count +=1\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will receive url of a submission, num_commnents as arg and it will return the top 10 comment\n",
    "# and mean comment score(upvote - downvote)\n",
    "\n",
    "def get_comments(url, num_comments):\n",
    "    # call api using api wrapper\n",
    "    subm = reddit.submission(url=url)\n",
    "    comments_body = []\n",
    "    sum_score = 0\n",
    "    count = 0\n",
    "    if num_comments > 0:\n",
    "        for i, comment in enumerate(subm.comments.list()):\n",
    "\n",
    "            # There may be some comments which has no body\n",
    "            try : \n",
    "                comments_body.append(comment.body)\n",
    "            except:\n",
    "                comments_body.append('')\n",
    "\n",
    "            # There exist some comments which has not given a score\n",
    "            try:\n",
    "                sum_score += comment.score\n",
    "            except:\n",
    "                sum_score += 0            \n",
    "            count += 1\n",
    "            # We only need 10 comments\n",
    "            if (i+1)%10==0:\n",
    "                break\n",
    "    try:\n",
    "        mean = sum_score/count\n",
    "    except:\n",
    "        mean = 0.0\n",
    "\n",
    "    return \" \".join(comments_body), mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call and retrive the data using pushshift api\n",
    "def getPushshiftSubData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=2000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties to extract about a submission\n",
    "features = [\n",
    "    'allow_live_comments', 'author', 'author_premium', 'can_mod_post', 'contest_mode', 'created_utc',\n",
    "    'full_link', 'id', 'is_crosspostable', 'is_meta', 'is_original_content', 'is_self', 'is_video',\n",
    "    'link_flair_text', 'locked', 'media_only', 'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
    "    'parent_whitelist_status', 'permalink', 'pinned', 'score', 'selftext', 'send_replies', 'spoiler',\n",
    "    'stickied', 'title', 'total_awards_received', 'url'\n",
    "]\n",
    "custom_features = [\n",
    "    'comments', 'mean_comment_score'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the features\n",
    "def collectSubData(subm):\n",
    "    # extracting all information about a submission\n",
    "    feature_values = [subm.get(key) for key in features]    \n",
    "    comments, mean_comment_score = get_comments(subm.get('full_link'), subm.get('num_comments')) \n",
    "    feature_values.append(comments)\n",
    "    feature_values.append(mean_comment_score)\n",
    "    subStats[subm.get('id')] = feature_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to extract the data\n",
    "#Subreddit to query\n",
    "sub='india'\n",
    "\n",
    "#before and after dates\n",
    "after = \"1583020800\"  # Sun, 01 Mar 2020 00:00:00 \n",
    "before = \"1586563199\" # Fri, 10 Apr 2020 23:59:59  \n",
    "query = \"\" # title should have either null string more \n",
    "subCount = 0\n",
    "subStats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?title=&size=2000&after=1583020800&before=1586563199&subreddit=india\n"
     ]
    }
   ],
   "source": [
    "data = getPushshiftSubData(query, after, before, sub)# Will run until all posts have been gathered \n",
    "# from the 'after' date up until before date\n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        collectSubData(submission)\n",
    "        subCount+=1\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    # print(len(data))\n",
    "    if subCount % 1000 == 0:\n",
    "        print(f'{subCount} submission collected')\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    after = data[-1]['created_utc']\n",
    "    data = getPushshiftSubData(query, after, before, sub)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subCount' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-798363cadec6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msubCount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'subCount' is not defined"
     ]
    }
   ],
   "source": [
    "subCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write extracted json data into pickle file\n",
    "with open(\"Submissions-March-2020.pkl\", \"wb\") as f:\n",
    "    pickle.dump(subStats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to pandas datafram\n",
    "df = pd.DataFrame(data=list(subStats.values()), columns = features + custom_features, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataframe into csv file\n",
    "df.to_csv(\"Submissions-March-2020.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python37064bitcdd344d0df034a7395ad747f8a4214da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
