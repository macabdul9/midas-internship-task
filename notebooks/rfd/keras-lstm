# encode the flair
le = LabelEncoder()
df['label']  = le.fit_transform(df.link_flair_text)

### Create sequence
vocabulary_size = 20000
tokenizer = Tokenizer(num_words= vocabulary_size)
tokenizer.fit_on_texts(df['text'])
sequences = tokenizer.texts_to_sequences(df['text'])
data = pad_sequences(sequences, maxlen=1000)

# split the data into train and test set
X_train, X_test, y_train, y_test = train_test_split(data, to_categorical(
    df['label'], num_classes=11, dtype='float32'
), test_size=0.1, shuffle=True)


print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

model = Sequential()
model.add(Embedding(vocabulary_size, 100, input_length=1000))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(11, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])



def load_and_prec():
    # if debug:
    #     train_df = pd.read_csv("../input/train.csv")[:80000]
    #     test_df = pd.read_csv("../input/test.csv")[:20000]
    # else:
    #     train_df = pd.read_csv("../input/train.csv")
    #     test_df = pd.read_csv("../input/test.csv")
    df = pd.read_csv("df.csv", usecols=['text', 'link_flair_text'])
    train_df = df[:-10000]
    test_df = df[-10000:]
    print("Train shape : ",train_df.shape)
    print("Test shape : ",test_df.shape)

    
    # lower the question text
    train_df["ext"] = train_df["text"].apply(lambda x: x.lower())
    test_df["text"] = test_df["text"].apply(lambda x: x.lower())

    # Clean the text
    train_df["text"] = train_df["text"].progress_apply(lambda x: clean_text(x))
    test_df["text"] = test_df["text"].apply(lambda x: clean_text(x))
    
    # Clean numbers
    train_df["text"] = train_df["text"].progress_apply(lambda x: clean_numbers(x))
    test_df["text"] = test_df["text"].apply(lambda x: clean_numbers(x))
    
    # Clean spellings
    train_df["text"] = train_df["text"].progress_apply(lambda x: replace_typical_misspell(x))
    test_df["text"] = test_df["text"].apply(lambda x: replace_typical_misspell(x))
    
    ## fill up the missing values
    train_X = train_df["text"].fillna("_##_").values
    test_X = test_df["text"].fillna("_##_").values

    ## Tokenize the sentences
    tokenizer = Tokenizer(num_words=max_features)
    tokenizer.fit_on_texts(list(train_X))
    train_X = tokenizer.texts_to_sequences(train_X)
    test_X = tokenizer.texts_to_sequences(test_X)

    ## Pad the sentences 
    train_X = pad_sequences(train_X, maxlen=maxlen)
    test_X = pad_sequences(test_X, maxlen=maxlen)

    ## Get the target values
    train_y = train_df['target'].values
    
    #shuffling the data
 
    np.random.seed(SEED)
    trn_idx = np.random.permutation(len(train_X))

    train_X = train_X[trn_idx]
    train_y = train_y[trn_idx]
    
    
    return train_X, test_X, train_y, tokenizer.word_index